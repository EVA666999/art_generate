fastapi>=0.104.1
uvicorn[standard]>=0.24.0
pydantic>=2.7.0
pydantic-settings>=2.0.0
httpx>=0.25.2
requests>=2.31.0
python-jose[cryptography]>=3.5.0
passlib[bcrypt]>=1.7.4
bcrypt>=3.2.0
PyJWT>=2.10.0
sqlalchemy[asyncio]>=2.0.0
asyncpg>=0.30.0
alembic>=1.16.0
databases>=0.9.0
python-dotenv>=1.0.0
python-multipart>=0.0.6
loguru>=0.7.0
tenacity>=9.1.2
python-telegram-bot>=20.0
pytest>=7.4.0
pytest-cov>=4.1.0
protobuf>=4.25.3

# Основные зависимости для работы с изображениями и системами
Pillow>=11.2.1
psutil>=5.9.0
numpy>=2.2.6

# Зависимости для машинного обучения и ИИ
torch>=2.7.1

# llama-cpp-python для работы с GGUF моделями
# Примечания по установке:
# 1. Для CUDA поддержки: pip install llama-cpp-python --force-reinstall --index-url https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/cu121
# 2. Для CPU только: pip install llama-cpp-python
# 3. Для Windows: используйте готовый wheel файл llama_cpp_python-0.2.89-cp310-cp310-win_amd64.whl
# 4. Для автоматической установки используйте скрипт install_llama_cpp.py

# Временная замена - будет установлена через скрипт или wheel
# llama-cpp-python==0.2.89

psycopg2-binary