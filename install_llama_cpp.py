#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA –∏–ª–∏ CPU.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–µ—Ä—Å–∏—é.
"""

import subprocess
import sys
import os
from typing import Optional, Tuple


def check_cuda_availability() -> Tuple[bool, Optional[str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä—Å–∏—é.
    
    Returns:
        Tuple[bool, Optional[str]]: (–¥–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ CUDA, –≤–µ—Ä—Å–∏—è CUDA)
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ nvidia-smi
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Ä—Å–∏—é CUDA –∏–∑ –≤—ã–≤–æ–¥–∞ nvidia-smi
            output = result.stdout
            if 'CUDA Version:' in output:
                for line in output.split('\n'):
                    if 'CUDA Version:' in line:
                        cuda_version = line.split('CUDA Version:')[1].strip()
                        return True, cuda_version
            return True, "unknown"
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        pass
    
    return False, None


def check_torch_cuda() -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π PyTorch CUDA.
    
    Returns:
        bool: True –µ—Å–ª–∏ PyTorch –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA
    """
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False


def install_llama_cpp_cuda(cuda_version: str = "cu121") -> bool:
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA.
    
    Args:
        cuda_version: –í–µ—Ä—Å–∏—è CUDA (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é cu121)
    
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
    """
    print(f"–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA {cuda_version}...")
    
    try:
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É
        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'llama-cpp-python', '-y'], 
                      check=False)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é —Å CUDA
        cmd = [
            sys.executable, '-m', 'pip', 'install', 'llama-cpp-python',
            '--force-reinstall',
            '--index-url', f'https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/{cuda_version}'
        ]
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("‚úÖ llama-cpp-python —Å CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ llama-cpp-python —Å CUDA: {e}")
        print(f"–í—ã–≤–æ–¥: {e.stdout}")
        print(f"–û—à–∏–±–∫–∏: {e.stderr}")
        return False


def install_llama_cpp_cpu() -> bool:
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç llama-cpp-python —Ç–æ–ª—å–∫–æ –¥–ª—è CPU.
    
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
    """
    print("–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é llama-cpp-python –¥–ª—è CPU...")
    
    try:
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É
        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'llama-cpp-python', '-y'], 
                      check=False)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º CPU –≤–µ—Ä—Å–∏—é
        cmd = [sys.executable, '-m', 'pip', 'install', 'llama-cpp-python', '--force-reinstall']
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("‚úÖ llama-cpp-python –¥–ª—è CPU —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ llama-cpp-python –¥–ª—è CPU: {e}")
        print(f"–í—ã–≤–æ–¥: {e.stdout}")
        print(f"–û—à–∏–±–∫–∏: {e.stderr}")
        return False


def test_llama_cpp_import() -> bool:
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–º–ø–æ—Ä—Ç llama_cpp.
    
    Returns:
        bool: True –µ—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ
    """
    try:
        import llama_cpp
        print(f"‚úÖ llama_cpp –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ! –í–µ—Ä—Å–∏—è: {llama_cpp.__version__}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –≤ llama_cpp
        if hasattr(llama_cpp, 'Llama'):
            print("‚úÖ –ö–ª–∞—Å—Å Llama –¥–æ—Å—Ç—É–ø–µ–Ω")
            return True
        else:
            print("‚ùå –ö–ª–∞—Å—Å Llama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False
            
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ llama_cpp: {e}")
        return False
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ llama_cpp: {e}")
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏."""
    print("üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ llama-cpp-python")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
    cuda_available, cuda_version = check_cuda_availability()
    torch_cuda_available = check_torch_cuda()
    
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda_available}")
    if cuda_available:
        print(f"–í–µ—Ä—Å–∏—è CUDA: {cuda_version}")
    print(f"PyTorch CUDA: {torch_cuda_available}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä—Å–∏—é CUDA –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏
    if cuda_version and cuda_version != "unknown":
        if "12.1" in cuda_version or "12.2" in cuda_version:
            cuda_suffix = "cu121"
        elif "12.0" in cuda_version:
            cuda_suffix = "cu120"
        elif "11.8" in cuda_version:
            cuda_suffix = "cu118"
        else:
            cuda_suffix = "cu121"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    else:
        cuda_suffix = "cu121"
    
    print(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å–∏—è CUDA: {cuda_suffix}")
    
    # –ü—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–µ—Ä—Å–∏—é —Å CUDA, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    if cuda_available and torch_cuda_available:
        print("\nüîÑ –ü—ã—Ç–∞—é—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–µ—Ä—Å–∏—é —Å CUDA...")
        if install_llama_cpp_cuda(cuda_suffix):
            if test_llama_cpp_import():
                print("\nüéâ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Ä—Å–∏—è —Å CUDA.")
                return
            else:
                print("\n‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å CUDA –ø—Ä–æ—à–ª–∞, –Ω–æ –∏–º–ø–æ—Ä—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ü—Ä–æ–±—É—é CPU –≤–µ—Ä—Å–∏—é...")
    
    # Fallback –Ω–∞ CPU –≤–µ—Ä—Å–∏—é
    print("\nüîÑ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é CPU –≤–µ—Ä—Å–∏—é...")
    if install_llama_cpp_cpu():
        if test_llama_cpp_import():
            print("\nüéâ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –≤–µ—Ä—Å–∏—è.")
            return
    
    print("\n‚ùå –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Ä—É—á–Ω—É—é:")
    print("–î–ª—è CUDA: pip install llama-cpp-python --force-reinstall --index-url https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/cu121")
    print("–î–ª—è CPU: pip install llama-cpp-python")
    sys.exit(1)


if __name__ == "__main__":
    main() 